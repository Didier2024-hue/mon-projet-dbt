ğŸ“˜ dbt + Snowflake â€“ Exam Migration Project

DataScientest | Data Engineer / Data Architect

ğŸ¯ Contexte & objectif

Ce projet consiste Ã  migrer lâ€™examen Snowflake (SQL) vers un workflow dbt, en appliquant les bonnes pratiques de modÃ©lisation, dâ€™orchestration et de documentation.
Lâ€™objectif est de dÃ©montrer une capacitÃ© Ã  industrialiser un pipeline analytique Snowflake dans un cadre dbt.

Le projet couvre :

la gestion de sources dans dbt,

la crÃ©ation de tables (schÃ©ma en Ã©toile),

la construction de vues analytiques (requÃªtes mÃ©tier),

lâ€™organisation du projet via tags dbt.

ğŸ§± Architecture du projet

Le projet respecte une architecture analytique classique :

1ï¸âƒ£ Sources (raw / staging)

Les donnÃ©es sont chargÃ©es depuis S3 vers Snowflake :

s3://mc-snowflake/sample/music/


Les tables sources sont rÃ©fÃ©rencÃ©es dans schema.yml via sources.

2ï¸âƒ£ ModÃ©lisation (star schema)

CrÃ©ation des tables fact et dimension via des fichiers .sql

ModÃ©lisation orientÃ©e BI (optimisÃ©e pour les requÃªtes analytiques)

3ï¸âƒ£ Consommation (vues analytiques)

Les requÃªtes de lâ€™examen sont implÃ©mentÃ©es en tant que vues dbt

Chaque vue est taguÃ©e pour une exÃ©cution ciblÃ©e

ğŸ“¦ Contenu du dÃ©pÃ´t
âœ… 1) Source configuration (schema.yml)

Le fichier schema.yml contient :

la dÃ©claration des sources

la localisation des tables Snowflake (database, schema, tables)

les tests optionnels (facultatifs mais recommandÃ©s)

ğŸ“Œ Objectif : lier dbt aux tables existantes dans Snowflake et garantir une documentation claire des sources.

âœ… 2) ModÃ¨les (tables du star schema)

Les modÃ¨les sont dÃ©finis dans le dossier models/ :

dim_artist.sql

dim_album.sql

dim_track.sql

dim_genre.sql

fact_track_play.sql (ou Ã©quivalent selon votre modÃ©lisation)

ğŸ“Œ Objectif : construire un schÃ©ma en Ã©toile prÃªt pour lâ€™analyse BI.

ğŸ”– Chaque modÃ¨le de crÃ©ation de table doit Ãªtre taguÃ© (ex : tags: ['create_tables']) afin dâ€™Ã©viter la recrÃ©ation Ã  chaque compilation.

âœ… 3) Vues analytiques (requÃªtes mÃ©tier)

Les requÃªtes de lâ€™examen sont implÃ©mentÃ©es en tant que vues dbt dans models/analytics/ :

Question	Objectif	Vue
3.1	Albums avec plus dâ€™un CD	v_albums_multi_cd.sql
3.2	Morceaux produits en 2000 ou 2002	v_tracks_2000_2002.sql
3.3	Morceaux Rock & Jazz (nom + compositeur)	v_rock_jazz_composers.sql
3.4	Top 10 albums les plus longs	v_top10_longest_albums.sql
3.5	Nombre dâ€™albums par artiste	v_albums_per_artist.sql
3.6	Nombre de morceaux par artiste	v_tracks_per_artist.sql
3.7	Genre le plus Ã©coutÃ© dans les annÃ©es 2000	v_top_genre_2000s.sql
3.8	Playlists avec morceaux > 4 min	v_playlists_long_tracks.sql
3.9	Rock tracks avec artistes en France	v_rock_tracks_france.sql
3.10	Moyenne durÃ©e des morceaux par genre	v_avg_track_length_by_genre.sql
3.11	Playlists avec artistes nÃ©s avant 1990	v_playlists_artists_pre1990.sql

ğŸ”– Chaque vue est Ã©galement taguÃ©e (ex : tags: ['exam_queries']) pour permettre une exÃ©cution ciblÃ©e.

ğŸš€ ExÃ©cution dbt (recommandÃ©e)
1) Charger les donnÃ©es dans Snowflake

(Ã‰tape prÃ©alable si non dÃ©jÃ  rÃ©alisÃ©e)

2) ExÃ©cuter dbt avec tags

Pour exÃ©cuter uniquement la crÃ©ation des tables :

dbt run --select tag:create_tables


Pour exÃ©cuter uniquement les vues analytiques :

dbt run --select tag:exam_queries

ğŸ§© Bonus (optionnel)

Des tests dbt peuvent Ãªtre ajoutÃ©s pour assurer la qualitÃ© des donnÃ©es, par exemple :

not_null

unique

relationships

ğŸ“Œ Conclusion

Ce projet dÃ©montre une approche industrielle de transformation SQL vers dbt, en structurant :

les sources,

la modÃ©lisation,

les requÃªtes mÃ©tier,

et lâ€™exÃ©cution via tags.

Il illustre une capacitÃ© Ã  industrialiser un pipeline analytique Snowflake, tout en respectant les bonnes pratiques dbt.
